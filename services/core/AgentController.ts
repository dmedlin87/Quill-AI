// services/core/AgentController.ts

import { ChatMessage, EditorContext, AnalysisResult, CharacterProfile } from '@/types';
import { Lore, Chapter } from '@/types/schema';
import { Persona } from '@/types/personas';
import { CritiqueIntensity } from '@/types/critiqueSettings';
import { ExperienceLevel, AutonomyMode } from '@/types/experienceSettings';
import { ManuscriptHUD } from '@/types/intelligence';
import type { UsageMetadata, Chat, FunctionCall } from '@google/genai';
import type { ToolResult } from '@/services/gemini/toolExecutor';
import { runAgentToolLoop, AgentToolLoopModelResult } from '@/services/core/agentToolLoop';
import { createChatSessionFromContext, buildInitializationMessage } from './agentSession';
import { ToolRunner } from './toolRunner.ts';

// ---- Shared with existing hook ----

export interface AgentState {
  status: 'idle' | 'thinking' | 'executing' | 'error';
  lastError?: string;
}

// ---- Static context passed in from the app (manuscript, settings, etc.) ----

export interface AgentContextInput {
  /** Full active chapter text (what useAgentService currently receives as fullText). */
  fullText: string;

  /** All chapters in the project, including which one is active. */
  chapters: Chapter[];

  /** Optional lore bible used to prime the agent. */
  lore?: Lore;

  /** Incremental or full analysis results, used for deep critique. */
  analysis?: AnalysisResult | null;

  /** Real-time intelligence HUD generated by the deterministic layer. */
  intelligenceHUD?: ManuscriptHUD;

  /** Optional interview target (character roleplay mode). */
  interviewTarget?: CharacterProfile;

  /** Project identifier for persistent memory context. */
  projectId?: string | null;

  /** Current critique intensity / “strictness”. */
  critiqueIntensity: CritiqueIntensity;

  /** Author experience level (used in prompt mod). */
  experienceLevel: ExperienceLevel;

  /** Autonomy mode (how proactive the agent should be). */
  autonomyMode: AutonomyMode;
}

// ---- Tool execution abstraction (wired to toolExecutor.ts or custom) ----

export interface AgentToolExecutor {
  execute(toolName: string, args: Record<string, unknown>): Promise<ToolResult>;
}

// ---- Memory abstraction (wraps getMemoriesForContext + getActiveGoals) ----

export interface MemoryProvider {
  /**
   * Returns a pre-formatted memory block for injection into the system prompt.
   */
  buildMemoryContext(projectId: string): Promise<string>;
}

// ---- Construction-time dependencies ----

export interface AgentControllerDependencies {
  /** Tool executor for app-side actions (navigation, editing, analysis, etc.). */
  toolExecutor: AgentToolExecutor;

  /** Optional memory provider (if omitted, memory is simply not injected). */
  memoryProvider?: MemoryProvider;
}

// ---- Event callbacks from the controller back to the UI layer ----

export interface AgentControllerEvents {
  /** Fired whenever the underlying agent state changes (idle/thinking/executing/error). */
  onStateChange?: (state: AgentState) => void;

  /** Fired when a new chat message (user or model) should be appended to UI state. */
  onMessage?: (message: ChatMessage) => void;

  /**
   * Fired when a tool call round starts.
   */
  onToolCallStart?: (payload: {
    id: string;
    name: string;
    args: Record<string, unknown>;
  }) => void;

  /**
   * Fired after a tool call completes (success or failure).
   */
  onToolCallEnd?: (payload: {
    id: string;
    name: string;
    result: ToolResult;
  }) => void;

  /** Fired for unrecoverable errors in the orchestration layer. */
  onError?: (error: Error) => void;
}

// ---- Streaming support ----

export interface AgentStreamChunk {
  /** Incremental text delta from the model, if streaming is enabled. */
  textDelta?: string;

  /** Convenience full accumulated text so far (optional). */
  fullText?: string;

  /** True when the model signals end-of-stream. */
  done?: boolean;

  /** Optional usage/telemetry from the underlying provider. */
  usage?: UsageMetadata;
}

export interface AgentStreamHandlers {
  /** Called for each incremental text chunk from the model. */
  onChunk: (chunk: AgentStreamChunk) => void;

  /** Called when the stream finishes successfully. */
  onComplete?: (finalChunk: AgentStreamChunk) => void;

  /** Called when the stream aborts or fails. */
  onError?: (error: Error) => void;
}

export interface SendMessageOptions {
  /** Optional external abort signal (e.g., from the hook or UI). */
  abortSignal?: AbortSignal;

  /** If provided, the controller will use streaming mode and call these handlers. */
  streamHandlers?: AgentStreamHandlers;
}

// ---- Public AgentController surface ----

export interface AgentController {
  /** Current persona used for responses. */
  getCurrentPersona(): Persona | undefined;

  /** Current internal state (idle/thinking/executing/error). */
  getState(): AgentState;

  /**
   * Initialize or reinitialize the underlying chat session.
   */
  initializeChat(persona: Persona, projectId?: string | null): Promise<void>;

  /**
   * Send a user message to the agent.
   */
  sendMessage(input: {
    text: string;
    editorContext: EditorContext;
    options?: SendMessageOptions;
  }): Promise<void>;

  /**
   * Explicit streaming helper (optional interface method).
   */
  sendMessageStreaming?(input: {
    text: string;
    editorContext: EditorContext;
    handlers: AgentStreamHandlers;
  }): Promise<void>;

  /**
   * Reset the chat session.
   */
  resetSession(): Promise<void>;

  /**
   * Hard cleanup: abort in-flight requests, release resources.
   */
  dispose(): void;

  /**
   * Change persona and (optionally) reinitialize the session.
   */
  setPersona(persona: Persona): Promise<void>;

  /**
   * Abort an in-flight request (tool loop or streaming).
   */
  abortCurrentRequest(): void;
}

// ---- Suggested concrete implementation signature ----

export interface AgentControllerConstructorArgs {
  context: AgentContextInput;
  deps: AgentControllerDependencies;
  events?: AgentControllerEvents;
  initialPersona?: Persona;
}

/**
 * Reference implementation that will live in this file.
 * NOTE: This is a skeleton; real Gemini + tool orchestration will be wired later.
 */
export class DefaultAgentController implements AgentController {
  private readonly context: AgentContextInput;
  private readonly deps: AgentControllerDependencies;
  private readonly events?: AgentControllerEvents;
  private readonly toolRunner: ToolRunner;
  private state: AgentState = { status: 'idle' };
  private currentPersona: Persona | undefined;
  private currentAbortController: AbortController | null = null;
  private chat: Chat | null = null;

  constructor(args: AgentControllerConstructorArgs) {
    this.context = args.context;
    this.deps = args.deps;
    this.events = args.events;
    this.currentPersona = args.initialPersona;

    this.toolRunner = new ToolRunner({
      toolExecutor: this.deps.toolExecutor,
      getProjectId: () => this.context.projectId ?? null,
      onMessage: this.events?.onMessage,
      onStateChange: state => this.updateState(state),
    });
  }

  private updateState(newState: Partial<AgentState>): void {
    this.state = { ...this.state, ...newState };
    this.events?.onStateChange?.(this.state);
  }

  getCurrentPersona(): Persona | undefined {
    return this.currentPersona;
  }

  getState(): AgentState {
    return this.state;
  }

  async initializeChat(persona: Persona, projectId?: string | null): Promise<void> {
    this.currentPersona = persona;
    this.updateState({ status: 'idle', lastError: undefined });

    const { chat, memoryContext } = await createChatSessionFromContext({
      context: this.context,
      persona,
      memoryProvider: this.deps.memoryProvider,
      projectId,
    });
    this.chat = chat;

    // Silent initialization message with persona and memory status (no UI message)
    this.chat
      ?.sendMessage({
        message: buildInitializationMessage({
          chapters: this.context.chapters,
          fullText: this.context.fullText,
          memoryContext,
          persona,
        }),
      })
      .catch(console.error);
  }

  async sendMessage(input: {
    text: string;
    editorContext: EditorContext;
    options?: SendMessageOptions;
  }): Promise<void> {
    if (!input.text.trim()) {
      return;
    }

    // Prevent overlapping requests until we explicitly support them.
    if (this.state.status === 'thinking' || this.state.status === 'executing') {
      return;
    }

    this.updateState({ status: 'thinking', lastError: undefined });
    this.toolRunner.resetTurn();

    const internalAbortController = new AbortController();
    this.currentAbortController = internalAbortController;

    const externalSignal = input.options?.abortSignal;
    if (externalSignal) {
      if (externalSignal.aborted) {
        internalAbortController.abort();
      } else {
        const onAbort = () => internalAbortController.abort();
        externalSignal.addEventListener('abort', onAbort, { once: true });
      }
    }

    // Ensure we have an initialized chat session
    if (!this.chat && this.currentPersona) {
      await this.initializeChat(this.currentPersona, this.context.projectId ?? null);
    }
    const chat = this.chat;
    if (!chat) {
      // If initialization still failed, bail out gracefully.
      this.updateState({ status: 'error', lastError: 'Agent session is not initialized.' });
      return;
    }

    // Emit user message immediately (mirrors original hook behavior).
    const userMessage: ChatMessage = {
      role: 'user',
      text: input.text,
      timestamp: new Date(),
    };
    this.events?.onMessage?.(userMessage);

    const abortSignal = internalAbortController.signal;

    try {
      const { streamHandlers } = input.options || {};
      if (streamHandlers) {
        // Streaming is not yet implemented; ignore handlers for now.
        console.warn('[AgentController] Streaming is not yet implemented.');
      }

      // Build context-aware prompt (ported from useAgentService)
      const { editorContext } = input;
      const contextPrompt = `
      [USER CONTEXT]
      Cursor Index: ${editorContext.cursorPosition}
      Selection: ${editorContext.selection ? `"${editorContext.selection.text}"` : 'None'}
      Total Text Length: ${editorContext.totalLength}
      
      [USER REQUEST]
      ${input.text}
      `;

      // Send to agent and run shared tool loop
      const initialResult = (await chat.sendMessage({
        message: contextPrompt,
      })) as AgentToolLoopModelResult;

      const finalResult = await runAgentToolLoop<AgentToolLoopModelResult>({
        chat,
        initialResult,
        abortSignal,
        processToolCalls: functionCalls => this.toolRunner.processToolCalls(functionCalls),
        onThinkingRoundStart: () => {
          this.updateState({ status: 'thinking', lastError: undefined });
        },
      });

      if (abortSignal.aborted) {
        return;
      }

      const result = finalResult;

      // Final text response
      const responseText = (result as any).text as string | undefined;
      const modelMessage: ChatMessage = {
        role: 'model',
        text: responseText || 'Done.',
        timestamp: new Date(),
      };
      this.events?.onMessage?.(modelMessage);

      await this.toolRunner.maybeSuggestBedsideNoteRefresh(
        `${input.text} ${responseText || ''}`,
      );

      this.updateState({ status: 'idle' });
    } catch (error) {
      if (error instanceof DOMException && error.name === 'AbortError') {
        // Aborts are treated as a clean cancellation, not an error state.
        this.updateState({ status: 'idle' });
      } else {
        this.events?.onError?.(error as Error);
        this.updateState({ status: 'error', lastError: (error as Error).message });
        // Mirror original hook: emit a friendly error message to the chat.
        const errorMessage: ChatMessage = {
          role: 'model',
          text: 'Sorry, I encountered an error connecting to the Agent.',
          timestamp: new Date(),
        };
        this.events?.onMessage?.(errorMessage);
      }
    } finally {
      this.currentAbortController = null;
    }
  }

  sendMessageStreaming(input: {
    text: string;
    editorContext: EditorContext;
    handlers: AgentStreamHandlers;
  }): Promise<void> {
    // Delegate to sendMessage with stream handlers until native streaming is wired.
    return this.sendMessage({
      text: input.text,
      editorContext: input.editorContext,
      options: { streamHandlers: input.handlers },
    });
  }

  async resetSession(): Promise<void> {
    this.abortCurrentRequest();
    this.updateState({ status: 'idle', lastError: undefined });
    // TODO: Reset underlying chat session once wired to Gemini layer.
  }

  dispose(): void {
    this.abortCurrentRequest();
    this.updateState({ status: 'idle', lastError: undefined });
    // TODO: Additional resource cleanup if the implementation requires it.
  }

  async setPersona(persona: Persona): Promise<void> {
    await this.initializeChat(persona, this.context.projectId ?? null);
  }

  abortCurrentRequest(): void {
    if (this.currentAbortController) {
      this.currentAbortController.abort();
      this.currentAbortController = null;
    }
    // Abort is treated as a neutral state transition back to idle.
    this.updateState({ status: 'idle' });
  }
}
